{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "tender-applicant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets\n"
     ]
    }
   ],
   "source": [
    "# Loading required packages\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from names_dataset import NameDataset\n",
    "from names_dataset import NameDatasetV1\n",
    "from flashtext import KeywordProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Select Spacy model\n",
    "# Efficiency\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Accuracy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# import stanza\n",
    "# stanza.download('en')       # This downloads the English models for the neural pipeline\n",
    "# nlp = stanza.Pipeline('en', processors=\"tokenize,ner\") # This sets up a default neural pipeline in English\n",
    "\n",
    "# Change working directory\n",
    "%cd '/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-kingston",
   "metadata": {},
   "source": [
    "## 2. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-output",
   "metadata": {},
   "source": [
    "### Load EMSCAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "unknown-order",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17880"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the csv file into the environment\n",
    "jobdescriptions = pd.read_csv('EMSCAD/Input data/JobDescriptions_cleaned.csv', delimiter=',')\n",
    "jobdescriptions\n",
    "\n",
    "# Check length dataframe\n",
    "len(jobdescriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-saturday",
   "metadata": {},
   "source": [
    "### Subset data, remain only the column description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "handled-purple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food, a fast-growing, -winning online food com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Organised - Focused - NAME_MASKED - Awesome!Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB TITLE: Itemization Review Manager \\nLOCATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>Just in case this is the first time you’ve vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17876</th>\n",
       "      <td>\\nThe Payroll Accountant will focus primarily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17877</th>\n",
       "      <td>Experienced Project Cost Control Staff Enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17878</th>\n",
       "      <td>NAME_MASKED Studios is looking for an experien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17879</th>\n",
       "      <td>Who are we? \\nNAME_MASKED is an award winning ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17880 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description\n",
       "0      Food, a fast-growing, -winning online food com...\n",
       "1      Organised - Focused - NAME_MASKED - Awesome!Do...\n",
       "2      Our client, located in Houston, is actively se...\n",
       "3      THE COMPANY: ESRI – Environmental Systems Rese...\n",
       "4      JOB TITLE: Itemization Review Manager \\nLOCATI...\n",
       "...                                                  ...\n",
       "17875  Just in case this is the first time you’ve vis...\n",
       "17876   \\nThe Payroll Accountant will focus primarily...\n",
       "17877  Experienced Project Cost Control Staff Enginee...\n",
       "17878  NAME_MASKED Studios is looking for an experien...\n",
       "17879  Who are we? \\nNAME_MASKED is an award winning ...\n",
       "\n",
       "[17880 rows x 1 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy column description from DF jobdescription\n",
    "descriptions = jobdescriptions['description']\n",
    "\n",
    "# Convert Series into Dataframe\n",
    "descriptions = descriptions.to_frame()\n",
    "descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-browser",
   "metadata": {},
   "source": [
    "## 3. Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-baker",
   "metadata": {},
   "source": [
    "### Remove HTML patterns in job descriptions\n",
    "\n",
    "### Once cleaned, we can put the data through Spacy's NLP pipeline and tokenize each description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "favorite-albuquerque",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.29 s, sys: 27.6 ms, total: 5.32 s\n",
      "Wall time: 5.37 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food, a fast-growing, -winning online food com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Organised - Focused - NAME_MASKED - Awesome!Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB TITLE: Itemization Review Manager  LOCATIO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Result\n",
       "0  Food, a fast-growing, -winning online food com...\n",
       "1  Organised - Focused - NAME_MASKED - Awesome!Do...\n",
       "2  Our client, located in Houston, is actively se...\n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...\n",
       "4  JOB TITLE: Itemization Review Manager  LOCATIO..."
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "result = []\n",
    "counter = 0\n",
    "\n",
    "Cleaned = pd.DataFrame()\n",
    "for i in descriptions['description']:\n",
    "    i = str(i)\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    i = re.sub(cleanr, '', i)\n",
    "    cleanr = re.compile('<[^>]+>')\n",
    "    i = re.sub(cleanr, '', i)\n",
    "    # Remove all between hashtag (# #)\n",
    "    i = re.sub(r'#[\\w-]+#', '#URL_MASKED#', i)\n",
    "    i = re.sub(r'#[\\w\\s-]+#', '#URL_MASKED#', i)\n",
    "    # URL universal\n",
    "    i = re.sub(r\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'.,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\", '#URL_MASKED#', i)\n",
    "    # URL pattern\n",
    "    i = re.sub(r'(#\\s*URL_).[^#]+\\s*[#]', '#URL_MASKED#', i)\n",
    "    # Email patten\n",
    "    i = re.sub(r'(#\\s*EMAIL_).[^#]+\\s*[#]', '#EMAIL_MASKED#', i)\n",
    "    # Email adresses\n",
    "    i = re.sub(r'/^([a-z0-9_\\.-]+\\@[\\da-z-]+\\.[a-z\\.]{2,6})$/', '#EMAIL_MASKED#', i)\n",
    "    # Phone pattern\n",
    "    i = re.sub(r'^[+]*[(]{0,1}[0-9]{1,4}[)]{0,1}[-\\s\\./0-9]*$', '#PHONE_MASKED#', i)\n",
    "    # All phone numbers\n",
    "    i = re.sub(r'[+]*[(]{0,1}[0-9]{1,4}[)]{0,1}[-\\s\\./0-9]*$', '#PHONE_MASKED#', i)\n",
    "    i = i.replace('\\xa0', ' ')\n",
    "    i = i.replace('\\r', ' ')\n",
    "    i = i.replace('\\n', ' ')\n",
    "    i = i.replace('&amp', ' ')\n",
    "    i = i.replace('\\N{SOFT HYPHEN}', '')\n",
    "    j = str(i)\n",
    "    i = str(i)\n",
    "    result.append(i)\n",
    "\n",
    "# Add the result\n",
    "Cleaned[\"Result\"] = result\n",
    "Cleaned.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-panel",
   "metadata": {},
   "source": [
    "### Split descriptions into sentences\n",
    "### By doing so, we prepare the data for annotation and training the custom NER model ---> Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "artificial-lloyd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.35 s, sys: 39 ms, total: 3.39 s\n",
      "Wall time: 3.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Split each description into sentences\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "endpoint = ('.')\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [token for token in sentences if not token.startswith(prefixes)]\n",
    "    return sentences\n",
    "\n",
    "def remove_invalid_sentences(sentences):\n",
    "    prefixes = ('.')\n",
    "    output = []\n",
    "    sentences = [token for token in sentences if not token.startswith(prefixes)]\n",
    "    output.append(sentences)\n",
    "    return output\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in Cleaned[\"Result\"]:\n",
    "    sentences = str(i)\n",
    "    sentences = split_into_sentences(sentences)\n",
    "#     sentences = remove_invalid_sentences(sentences)\n",
    "    if sentences:\n",
    "        result.append(sentences)\n",
    "\n",
    "sentences = pd.DataFrame(columns=['sentence'])\n",
    "sentences[\"sentence\"] = result\n",
    "sentences['sentence'].to_csv('EMSCAD/Output data/sentence.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-boulder",
   "metadata": {},
   "source": [
    "### TRAIN/ TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "brazilian-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% / 20% split\n",
    "Train, Eval = train_test_split(sentences, test_size=0.2, shuffle=False)\n",
    "Train = list(Train['sentence'])\n",
    "Eval = list(Eval['sentence'])\n",
    "FULL = list(sentences['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-america",
   "metadata": {},
   "source": [
    "## 4. Preparation and annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "sixth-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 481/17374 [02:34<1:40:40,  2.80it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (882 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 17374/17374 [1:39:48<00:00,  2.90it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6h 18min 42s, sys: 8min 57s, total: 6h 27min 40s\n",
      "Wall time: 1h 39min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Train_Annotation_data = pd.DataFrame(columns=['Result', 'Label'])\n",
    "Eval_Annotation_data = pd.DataFrame(columns=['Result', 'Label'])\n",
    "FULL_Annotation_data = pd.DataFrame(columns=['Result', 'Label'])\n",
    "#########################\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "#########################\n",
    "\n",
    "prefixes = [\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\" \",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\"]\n",
    "prefixes_end = [\"?\",\"!\"]\n",
    "full_stop = [\".\"]\n",
    "gdpr_begin = [\"URL\", \"#URL\", \"EMAIL\", \"#EMAIL\", \"PHONE\", \"#PHONE\"]\n",
    "gdpr_begin = \"(URL|#URL|EMAIL|#EMAIL|PHONE|#PHONE)\"\n",
    "\n",
    "def sentence_to_words(input_list):\n",
    "    prefixes = [\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\" \",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\"]\n",
    "    prefixes_end = [\"?\",\"!\"]\n",
    "    full_stop = [\".\"]\n",
    "    result = []\n",
    "    max_length = 0\n",
    "    count = 0\n",
    "    for i in tqdm(input_list):\n",
    "        i = str(i)\n",
    "        ######################### USE FOR en_core_web_trf model only! (because this model can handle only sentences with the number of characters up to 512)\n",
    "        if len(i) > max_length:\n",
    "            max_length = len(i)\n",
    "        if len(i)>512:\n",
    "            i = i[:512]\n",
    "            count = count +1\n",
    "        #########################\n",
    "        i = nlp(i)\n",
    "        for token in i:\n",
    "            #########################\n",
    "            token = token.lemma_\n",
    "            #########################\n",
    "            if str(token) not in prefixes and str(token) not in prefixes_end:\n",
    "                result.append(token)\n",
    "            elif str(token) in prefixes_end:\n",
    "                result.append(nlp(full_stop[0]))\n",
    "#     print(count)\n",
    "#     print(max_length)\n",
    "    return result\n",
    "\n",
    "Train_Annotation_data[\"Result\"] = sentence_to_words(Train)\n",
    "Eval_Annotation_data[\"Result\"] = sentence_to_words(Eval)\n",
    "FULL_Annotation_data[\"Result\"] = sentence_to_words(FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "chemical-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to be annotated (Can be used for manual annotation!)\n",
    "Train_Annotation_data['Result'].to_csv('EMSCAD/Output data/Train_Annotation_data.csv')\n",
    "Eval_Annotation_data['Result'].to_csv('EMSCAD/Output data/Eval_Annotation_data.csv')\n",
    "FULL_Annotation_data['Result'].to_csv('EMSCAD/Output data/Full_Annotation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "answering-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the source file that contains all biased word lists\n",
    "biased_words = pd.read_csv('EMSCAD/Input data/biased_words.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fiscal-calibration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2977570/2977570 [36:38<00:00, 1354.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117200 words have been annotated.\n",
      "CPU times: user 36min 17s, sys: 20.3 s, total: 36min 38s\n",
      "Wall time: 36min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Automated annotation process (Based on the word lists imported.)\n",
    "# Only exact matches will be annotated.\n",
    "def automated_annotation(Annotation_data):\n",
    "    result = []\n",
    "    row = -1\n",
    "    count = 0\n",
    "    for i in tqdm(Annotation_data['Result']):\n",
    "        i = str(i)\n",
    "        row = row + 1\n",
    "        for j in biased_words:\n",
    "            for k in biased_words[j]:\n",
    "                word = str(k)\n",
    "                if word == i:\n",
    "                    Annotation_data['Label'][row] = j\n",
    "                    count = count + 1\n",
    "    Annotation_data['Label'] = Annotation_data['Label'].fillna(\"O\")\n",
    "    print(str(count) + \" words have been annotated.\")\n",
    "    return Annotation_data\n",
    "\n",
    "Train_Annotation_data = automated_annotation(Train_Annotation_data)\n",
    "Eval_Annotation_data = automated_annotation(Eval_Annotation_data)\n",
    "FULL_Annotation_data = automated_annotation(FULL_Annotation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "smoking-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export annotated data (By \"automated\" annotator)\n",
    "Train_Annotation_data.to_csv('EMSCAD/Output data/Train_Annotation_data_output.tsv', sep='\\t', index = False, header = False)\n",
    "Eval_Annotation_data.to_csv('EMSCAD/Output data/Eval_Annotation_data_output.tsv', sep='\\t', index = False, header = False)\n",
    "FULL_Annotation_data.to_csv('EMSCAD/Output data/FULL_Annotation_data_output.tsv', sep='\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "surprising-oakland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                                  2909508\n",
       "Masculine-coded words                37572\n",
       "Feminine-coded words                 31911\n",
       "Exclusive language                    2765\n",
       "Demographic and Racial language        380\n",
       "LGBTQ-coloured language                103\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model config 1 and 2 --> Dataset 1\n",
    "FULL_Annotation_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "analyzed-removal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                                  2866384\n",
       "Feminine-coded words                 54143\n",
       "Masculine-coded words                52094\n",
       "Exclusive language                    3883\n",
       "Demographic and Racial language       1694\n",
       "LGBTQ-coloured language                172\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model config 3 and 4 -->  Dataset 2\n",
    "FULL_Annotation_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "narrative-batch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                                  2909508\n",
       "Masculine-coded words                37572\n",
       "Feminine-coded words                 31911\n",
       "Exclusive language                    2765\n",
       "Demographic and Racial language        380\n",
       "LGBTQ-coloured language                103\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model config 5 and 6 -->  Dataset 3\n",
    "FULL_Annotation_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "danish-directive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                                  2862507\n",
       "Feminine-coded words                 54898\n",
       "Masculine-coded words                54311\n",
       "Exclusive language                    4022\n",
       "Demographic and Racial language       1658\n",
       "LGBTQ-coloured language                174\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model config 7 and 8 -->  Dataset 4\n",
    "FULL_Annotation_data['Label'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
